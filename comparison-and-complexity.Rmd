---
title: "CTA-ED Exercise 3: Comparison and complexity"
author: "Ziyi Wei"
date: "2/12/2024"
output: html_document
---

## Introduction

The hands-on exercise for this week focuses on: 1) comparing texts; 2) measuring he document-level characteristics of text---here, complexity.

In this tutorial, you will learn how to:
  
* Compare texts using character-based measures of similarity and distance
* Compare texts using term-based measures of similarity and distance
* Calculate the complexity of texts
* Replicate analyses from @schoonvelde_liberals_2019

## Setup 

Before proceeding, we'll load the remaining packages we will need for this tutorial.

```{r, echo=F}
library(kableExtra)
```

```{r, message=F}
library(readr) # more informative and easy way to import data
library(quanteda) # includes functions to implement Lexicoder
library(quanteda.textstats) # for estimating similarity and complexity measures
library(stringdist) # for basic character-based distance measures
library(dplyr) #for wrangling data
library(tibble) #for wrangling data
library(ggplot2) #for visualization
```


For this example we'll be using data from the 2017-2018 Theresa May Cabinet in the UK. The data are tweets by members of this cabinet. 

```{r}
tweets  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/comparison-complexity/cabinet_tweets.rds?raw=true")))
```

And we see that the data contain three variables: "username," which is the username of the MP in question; "tweet," which is the text of the given tweet, and "date" in days in yyyy-mm-dd format. 

```{r}
head(tweets)
```

And there are 24 MPs whose tweets we're examining. 

```{r}
unique(tweets$username)

length(unique(tweets$username))
```

## Generate document feature matrix

In order to use the `quanteda` package and its accompanying `quanteda.textstats` package, we need to reformat the data into a quanteda "corpus" object. To do this we just need to specify the text we're interested in as well as any associated document-level variables in which we're interested. 

We can do this as follows. 

```{r}
#make corpus object, specifying tweet as text field
tweets_corpus <- corpus(tweets, text_field = "tweet")

#add in username document-level information
docvars(tweets_corpus, "username") <- tweets$username

head(tweets_corpus)
```

We are now ready to reformat the data into a document feature matrix.

```{r}
dfmat <- dfm(tokens(tweets_corpus, remove_punct = TRUE)) %>% dfm_remove(pattern = stopwords("english"))

head(dfmat)
```

Note that when we do this we need to have tokenized our corpus object first. We can do this by wrapping the `tokens` function inside the `dfm()` function as above. 

So what is this object? Well the documents here are tweets. And the matrix is a sparse (i.e., mostly zeroes) matrix of 1s and 0s for whether a given word appears in the document (tweet) in question. 

The vertical elements (columns) of this vector are made up of all the words used in all of the tweets combined. Here, it helps to imagine every tweet positioned side by side to understand what's going on here. 

## Compare between MPs

Once we have our data in this format, we are ready to compare between the text produced by members of Theresa May's Cabinet.

Here's an example of the correlations between the combined tweets of 5 of the MPs with each other.

```{r}
corrmat <- dfmat %>%
  dfm_group(groups = username) %>%
  textstat_simil(margin = "documents", method = "correlation")

corrmat[1:5,1:5]

```

Note that here we're using the `dfm_group()` function, which allows you to take a document feature matrix and make calculations while grouping by one of the document-level variables we specified above. 

There are many different measures of similarity, however, that we might think about using.

In the below, we combine four different measures of similarity, and see how they compare to each other across MPs. Note that here we're looking only at the similarity between an MP's tweets and those of then Prime Minister, Theresa May.

## Compare between measures

Let's see what this looks like for one of these measures---cosine similarity.

We first get similarities between the text of MP tweets and all other MPs.

```{r}

#estimate similarity, grouping by username

cos_sim <- dfmat %>%
  dfm_group(groups = username) %>%
  textstat_simil(margin = "documents", method = "cosine") #specify method here as character object

```

But remember we're only interested in how they compare to what Theresa May has been saying. 

So we need to take these cosine similarities and retain only those similarity measures corresponding to the text of Theresa May's tweets. 

We first convert the `textstat_simil()` output to a matrix.

```{r}

cosmat <- as.matrix(cos_sim) #convert to a matrix
head(cosmat)
  
```

And we can see that the 23rd row of this matrix contains the similarity measures with the Theresa May tweets. 

We take this row, removing the similarity of Theresa May with herself (which will always = 1), and convert it to a datframe object. 

```{r}
#generate data frame keeping only the row for Theresa May
cosmatdf <- as.data.frame(cosmat[23, c(1:22, 24)])
```

We then rename the cosine similarity column with an appropriate name and convert row names to a column variable so that we have cells containing information on the MP to which the cosine similarity measure refers. 

```{r}
#rename column
colnames(cosmatdf) <- "corr_may"
  
#create column variable from rownames
cosmatdf <- tibble::rownames_to_column(cosmatdf, "username")
```

And like so we have our data in tidy format, which we can then plot like so. 

```{r}
ggplot(cosmatdf) +
  geom_point(aes(x=reorder(username, -corr_may), y= corr_may)) + 
  coord_flip() +
  xlab("MP username") +
  ylab("Cosine similarity score") + 
  theme_minimal()

```

Combining these steps into a single `for` loop, we can see how our different similarity measures of interest compare. 

```{r}

#specify different similarity measures to explore
methods <- c("correlation", "cosine", "dice", "edice")

#create empty dataframe
testdf_all <- data.frame()

#gen for loop across methods types
for (i in seq_along(methods)) {
  
  #pass method to character string object
  sim_method <- methods[[i]]
  
  #estimate similarity, grouping by username
  test <- dfmat %>%
    dfm_group(groups = username) %>%
    textstat_simil(margin = "documents", method = sim_method) #specify method here as character object created above
  
  testm <- as.matrix(test) #convert to a matrix
  
  #generate data frame keeping only the row for Theresa May
  testdf <- as.data.frame(testm[23, c(1:22, 24)])
  
  #rename column
  colnames(testdf) <- "corr_may"
  
  #create column variable from rownames
  testdf <- tibble::rownames_to_column(testdf, "username")
  
  #record method in new column variable
  testdf$method <- sim_method

  #bind all together
  testdf_all <- rbind(testdf_all, testdf)  
  
}

#create variable (for viz only) that is mean of similarity scores for each MP
testdf_all <- testdf_all %>%
  group_by(username) %>%
  mutate(mean_sim = mean(corr_may))

ggplot(testdf_all) +
  geom_point( aes(x=reorder(username, -mean_sim), y= corr_may, color = method)) + 
  coord_flip() +
  xlab("MP username") +
  ylab("Similarity score") + 
  theme_minimal()

```

## Complexity

We now move to document-level measures of text characteristics. And here we will focus on the paper by @schoonvelde_liberals_2019. 

We will be using a subset of these data, taken from EU speeches given by four politicians. These are provided by the authors at [https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/S4IZ8K](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/S4IZ8K).

We can load the data as follows.

```{r}

speeches  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/comparison-complexity/speeches.rds?raw=true")))

```

And we can take a look at what the data contains below.

```{r, eval=F}

head(speeches)

```

The data contain speeches by four different politicians, each of whom are positioned at different points on a liberal-conservative scale.

We can then calculate the Flesch-Kincaid readability/complexity score with the `quanteda.textstats` package like so.

```{r}
speeches$flesch.kincaid <- textstat_readability(speeches$text, measure = "Flesch.Kincaid")

# returned as quanteda data.frame with document-level information;
# need just the score:
speeches$flesch.kincaid <- speeches$flesch.kincaid$Flesch.Kincaid
```

We want this information aggregated over each of our politicians: Gordon Brown, Jose Zapatero", David Cameron, and Mariano Rajoy. These are recorded in the data under a column called "speaker."

```{r}
#get mean and standard deviation of Flesch-Kincaid, and N of speeches for each speaker
sum_corpus <- speeches %>%
  group_by(speaker) %>%
  summarise(mean = mean(flesch.kincaid, na.rm=TRUE),
                   SD=sd(flesch.kincaid, na.rm=TRUE),
                   N=length(speaker))

# calculate standard errors and confidence intervals
sum_corpus$se <- sum_corpus$SD / sqrt(sum_corpus$N)
sum_corpus$min <- sum_corpus$mean - 1.96*sum_corpus$se
sum_corpus$max <- sum_corpus$mean + 1.96*sum_corpus$se
```

And this gives us data in tidy format that looks like so. 

```{r}
sum_corpus
```

Which we can then plot---and we see that our results look like those in Figure 1 of the published article by @schoonvelde_liberals_2019. 


```{r}

ggplot(sum_corpus, aes(x=speaker, y=mean)) +
  geom_bar(stat="identity") + 
  geom_errorbar(ymin=sum_corpus$min,ymax=sum_corpus$max, width=.2) +
  coord_flip() +
  xlab("") +
  ylab("Mean Complexity") + 
  theme_minimal() + 
  ylim(c(0,20))

```

## Exercises

1. Compute distance measures such as "euclidean" or "manhattan" for the MP tweets as above, comparing between tweets by MPs and tweets by PM, Theresa May. 

Use dist() function from proxy package to calculate the euclidean and manhattan distance between the MPs and the PM. The results show that there are some differences between these two methods. For example, trussliz has a relatively low similarity to the PM based on the Euclidean distance but holds a middle position in similarity when measured using the Manhattan distance.

```{r}
# Import proxy to use dist() function
library(proxy)
```

```{r}
# convert dfm to matrix and grouping by username
dfm_matrix <- dfmat %>%
    dfm_group(groups = username) %>%
    as.matrix()

# calculate the Euclidean distance
euclidean_dist <- proxy::dist(dfm_matrix, method = "Euclidean")
# choose the 23rd row and all columns except the 23rd column and convert it to a data frame
euclidean_df_PM <- as.matrix(euclidean_dist)[23, c(1:22, 24)] %>% as.data.frame()
```

```{r}
#rename column
colnames(euclidean_df_PM) <- "corr_may"
  
#create column variable from rownames
euclidean_df_PM <- tibble::rownames_to_column(euclidean_df_PM, "username")

head(euclidean_df_PM)
```

```{r}
# plot the Euclidean distance
ggplot(euclidean_df_PM) +
  geom_point(aes(x=reorder(username, -corr_may), y= corr_may)) + 
  coord_flip() +
  xlab("MP username") +
  ylab("Euclidean distance score") + 
  theme_minimal()
```
```{r}
# calculate the Manhattan distance
manhattan_dist <- proxy::dist(dfm_matrix, method = "Manhattan")
manhattan_df_PM <- as.matrix(manhattan_dist)[23, c(1:22, 24)] %>% as.data.frame()
```

```{r}
#rename column
colnames(manhattan_df_PM) <- "corr_may"
  
#create column variable from rownames
manhattan_df_PM <- tibble::rownames_to_column(manhattan_df_PM, "username")

head(manhattan_df_PM)
```

```{r}
ggplot(manhattan_df_PM) +
  geom_point(aes(x=reorder(username, -corr_may), y= corr_may)) + 
  coord_flip() +
  xlab("MP username") +
  ylab("Manhattan distance score") + 
  theme_minimal()
```


2. Estimate at least three other complexity measures for the EU speeches as above. Consider how the results compare to the Flesch-Kincaid measure used in the article by @schoonvelde_liberals_2019.

Complexity measures:Flesch-Kincaid Grade Level, SMOG Index, Flesch Reading Ease, Dale-Chall Readability Score
Ranking:
- Flesch-Kincaid: J.L.R. Zapatero > M. Rajoy > G. Brown > D. Cameron
- SMOG: J.L.R. Zapatero > M. Rajoy > G. Brown > D. Cameron
- Flesch: J.L.R. Zapatero > M. Rajoy > G. Brown > D. Cameron
- Dale.Chall: J.L.R. Zapatero > M. Rajoy > G. Brown > D. Cameron
(different complexity measures show similar results)
```{r}
# Complexity measures to calculate and compare
# Flesch-Kincaid meaning the higher the score, the easier the text is to read
# SMOG meaning the higher the score, the more difficult the text is to read
# Flesch meaning the higher the score, the easier the text is to read
# The New Dale.Chall meaning the higher the score, the easier the text is to read

measures_com <- c("Flesch.Kincaid", "SMOG", "Flesch", "Dale.Chall")

# Create an empty data frame to store results
all_results_com <- data.frame()

# Loop through each measure to calculate complexity and summarize results
for (measure in measures_com) {
  
  # Calculate complexity scores
  readability_scores <- textstat_readability(speeches$text, measure = measure)
  
  # Add the scores to the speeches data frame
  speeches[[measure]] <- readability_scores[[measure]]
  
  # Summarize mean, SD, N, and confidence intervals per speaker
  sum_corpus_com <- speeches %>%
    group_by(speaker) %>%
    summarise(
      mean = mean(get(measure), na.rm = TRUE),
      SD = sd(get(measure), na.rm = TRUE),
      N = n()
    ) %>%
    mutate(
      se = SD / sqrt(N),
      min = mean - 1.96 * se,
      max = mean + 1.96 * se,
      measure = measure
    )
  
  # Append to the results data frame
  all_results_com <- bind_rows(all_results_com, sum_corpus_com)
}

```

```{r}
# Plotting the results
ggplot(all_results_com, aes(x = speaker, y = mean, fill = measure)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9)) + # places bars for different groups side-by-side instead of stacking them
  geom_errorbar(aes(ymin = min, ymax = max), width = 0.2, position = position_dodge(width = 0.9)) +
  coord_flip() +
  xlab("") +
  ylab("Mean Complexity Score") +
  labs(title = "Mean Complexity Scores with 95% Confidence Intervals",
       fill = "Complexity Measure") +
  theme_minimal()
```

3. (Advanced---optional) Estimate similarity scores between the MP tweets and the PM tweets for each week contained in the data. Plot the results. 

The results show that the similarity scores between MPs and the PM vary over time. Highest around the end of 2017 and the beginning of 2018, and lowest around Sep of 2017. Almost all the similarity scores of the MPs fluctuate significantly over time.
```{r}
# Import lubridate to manipulate dates
library(lubridate)
# import pivot_longer() function from tidyr package
library(tidyr)
```

```{r}
# convert date to week
docvars(dfmat, "week") <- floor_date(docvars(dfmat, "date"), unit = "week") # floor_date() function from lubridate package to round down to the nearest week
head(docvars(dfmat))
```

```{r}
#specify different similarity measures to explore
methods_sim_week <- c("correlation", "cosine")

#create empty dataframe
testdf_all_week <- data.frame()

#gen for loop across methods types
for (i in seq_along(methods_sim_week)) {
  
  # pass method to character string object
  sim_method_week <- methods_sim_week[[i]]
  
  # group by username and week
  test_week <- dfmat %>%
    dfm_group(groups = interaction(username, week, sep = "_")) %>%
    textstat_simil(margin = "documents", method = sim_method_week)
  
  testm_week <- as.matrix(test_week) #convert to a matrix
  
  # Convert matrix to a long-format dataframe for plotting
  testdf_week <- as.data.frame(testm_week) %>%
    tibble::rownames_to_column("doc1") %>%
    pivot_longer(-doc1, names_to = "doc2", values_to = "similarity") # Convert wide to long formatï¼›doc1 is the row name, doc2 is the column name
  
  # filter pairs where one document corresponds to "theresa_may"
  testdf_week <- testdf_week %>%
    filter(grepl("^theresa_may_", doc1) & !grepl("^theresa_may_", doc2))  # comparing theresa_may to others; filter rows where 'doc1' starts with "theresa_may_" and 'doc2' does not
  
  # extract 'username' and 'week' from docnames
  testdf_week <- testdf_week %>%
    mutate(
      week = sub(".*_", "", doc1),  # Extract week from doc1 (theresa_may_xxx), remove everything before the last "_"
      username = sub("_(\\d{4}-\\d{2}-\\d{2})$", "", doc2)  # Extract username from doc2, remove everything after the last "_"
    ) %>%
    mutate(
      week = as.Date(week),
      method = sim_method_week
    ) %>%
    select(username, week, similarity, method)
  
  # append results
  testdf_all_week <- bind_rows(testdf_all_week, testdf_week)
}
```

```{r}
# calculate mean similarity for plotting
testdf_all_week <- testdf_all_week %>%
  group_by(username, week) %>%
  mutate(mean_sim = mean(similarity, na.rm = TRUE))
```

```{r, fig.width=12, fig.height=16}
# plot similarity trends over weeks
ggplot(testdf_all_week, aes(x = week, y = mean_sim, color = username)) +
  geom_line(aes(group = username), size = 0.2) +
  geom_point(size = 0.8) +
  facet_wrap(~ method, ncol = 1) + # Separate plots by method, 1 column
  coord_flip() +
  scale_x_date(
    expand = expansion(mult = c(0.1, 0.1)),  # Add space before and after the axis
    date_breaks = "1 month",  
    date_labels = "%b %Y"  # Format as "Jan 2024", etc.
  ) +
  labs(
    title = "Weekly Similarity Between MPs and Theresa May",
    x = "Week",
    y = "Similarity Score",
    color = "MP Username"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```
